{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=LsK-xG1cLYA&list=PL7BDbUvVSRsfh1BATt-JtDsYixO19Hz0D&ab_channel=StatQuestwithJoshStarmer\n",
    "\n",
    "AdaBoost 是以Decision tree和random forest 作为基础的\n",
    "\n",
    "一个node 和两个leaves构成一个stump\n",
    "AdaBoost 由很多stump构成\n",
    "\n",
    "相比于random forest\n",
    "random forest由树构成，在random forest中每棵树的权重都相同，尽管每棵树可能长得完全不同；顺序不重要\n",
    "AdaBoost 由很多stump构成，AdaBoost的Forest of Stumps中 每个stump的权重都不一样；顺序很重要\n",
    "\n",
    "AdaBoost的步骤\n",
    "1. 首先给每一个sample一个weight 代表这个sample被正确分类的重要程度 一开始所有的weight都是均衡的\n",
    "2. 然后查看每个column（variable）用来作为分类依据的Gini Index（注1）， 选择最小的那个variable作为Forest 的第一个stump\n",
    "3. 计算stump的total error，再用公式amount of say = 0.5log((1-total error)/total error)计算amount of say，比如当total error为1/8的时候，amount of say是0.97；total error越大，amount of say 越小，amount of say越大表示stump分类能力越好，错误项在下一个stump的权重就越大\n",
    "4. 用amount of say来计算每个sample的new sample weight，再进行normalization\n",
    "5. 抽样根据weights构成新的samples给下一个stump用（也可以用weighted gini index的方法，具体怎么样我也不知道），实际上下一个stump的权重都还是平均的，但因为有一部分是相同的，相当于是一种weight，这样就不停构成下一个stump\n",
    "6. 将所有的stump给出的不同结果结果的amount of say 统计加和，哪一个大就选哪一个\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "注1：如果有variable是continuous的， 则用某一个合理的分界值作为分类依据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
